{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'authentication'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[519], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mauthentication\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcredential\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mCR\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'authentication'"
     ]
    }
   ],
   "source": [
    "import authentication.credential as CR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import authentication.credential as cr\n",
    "NOTION_TOKEN = CR.NOTION_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://api.notion.com/v1/\"\n",
    "base_headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Notion-Version\": \"2022-06-28\",\n",
    "    \"Authorization\": NOTION_TOKEN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search(payload) searches all original pages, databases, and child pages/databases that are shared with the integration.\n",
    "def search(payload: dict={}):\n",
    "    url = base_url + \"search\"\n",
    "    headers = base_headers\n",
    "    headers.update({\"content-type\": \"application/json\"})\n",
    "\n",
    "    response = requests.post(url=url, json=payload, headers=headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database(database_id) retrieves a Database object using the ID specified.\n",
    "def database(database_id: str):\n",
    "    url = base_url + \"databases/\" + database_id\n",
    "\n",
    "    response = requests.get(url=url, headers=base_headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_database(database_id, payload) gets a list of Pages contained in the database, \n",
    "# fitered and ordered according to the filter conditions and sort criteria provided in the request.\n",
    "def query_database(database_id: str, payload: dict = {}):\n",
    "    url = base_url + \"databases/\" + database_id + \"/query\"\n",
    "    headers = base_headers\n",
    "    headers.update({\"content-type\": \"application/json\"})\n",
    "\n",
    "    response = requests.post(url=url, json=payload, headers=headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page(page_id) retrieves a Page object using the ID specified.\n",
    "def page(page_id: str):\n",
    "    url = base_url + \"pages/\" + page_id\n",
    "\n",
    "    response = requests.get(url=url, headers=base_headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block(block_id) retrieves a Block object using the ID specified.\n",
    "def block(block_id: str):\n",
    "    url = base_url + \"blocks/\" + block_id\n",
    "\n",
    "    response = requests.get(url=url, headers=base_headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_children(block_id) returns a paginated array of children block objects contained in the block using\n",
    "# the ID specified.\n",
    "def block_children(block_id: str):\n",
    "    url = base_url + \"blocks/\" + block_id + \"/children\"\n",
    "\n",
    "    response = requests.get(url=url, headers=base_headers)\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_timetables() returns a list of Block objects for Timetable databases\n",
    "def get_timetables():\n",
    "\n",
    "    # Page ID for 'Year 2023' that contains the WEEKS pages.\n",
    "    page_id = \"513f9a9b1ebc44f28a632db0b88c2ac7\"\n",
    "\n",
    "    # filter the 'Year 2023' page for WEEKS pages\n",
    "    filter_for_weeks = lambda x: (x['type'] == 'child_page') and ('WEEK' in x['child_page']['title'])\n",
    "    children = filter(filter_for_weeks, block_children(page_id)['results'])\n",
    "\n",
    "    # filter the WEEKS pages for Timetable databases\n",
    "    filter_for_timetables = lambda x: (x['type'] == 'child_database') and (x['child_database']['title'] == 'Timetable')\n",
    "    result = [(child['child_page']['title'], next(filter(filter_for_timetables, block_children(child['id'])['results']))['id']) for child in children]\n",
    "\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_action(data: list):\n",
    "    return \"\" if data == [] else data[0][\"text\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notion_to_pandas(database_id: str):\n",
    "\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    week = query_database(database_id, payload={\"sorts\":[{\"property\": \"Time\", \"direction\": \"ascending\"}]})['results']\n",
    "    data = list(map(lambda x: [x[\"properties\"][\"Time\"][\"title\"][0][\"text\"][\"content\"]] + [extract_action(x[\"properties\"][day][\"rich_text\"]) for day in days], week))\n",
    "    return pd.DataFrame(data, columns=(['Time'] + days))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PocketBase API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "# from pocketbase import PocketBase (This library is currently out-dated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://127.0.0.1:8090\"\n",
    "ADMIN_EMAIL = CR.POCKETBASE_ADMIN_EMAIL\n",
    "ADMIN_PASSWORD = CR.POCKETBASE_ADMIN_PASSWORD\n",
    "POCKETBASE_AUTH_TOKEN = CR.POCKETBASE_AUTH_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def admin_auth_with_password():\n",
    "\n",
    "    url = base_url + \"/api/admins/auth-with-password\"\n",
    "    body = {\"identity\": ADMIN_EMAIL, \"password\": ADMIN_PASSWORD}\n",
    "    response = requests.post(url=url, json=body, params={}, headers={})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_levels(level: str, params: dict = {}, full: bool = True):\n",
    "\n",
    "    url = base_url + f\"/api/collections/{level}/records\"\n",
    "    headers = {\"Authorization\": POCKETBASE_AUTH_TOKEN}\n",
    "    response = requests.get(url=url, params=params, headers=headers)\n",
    "    if (full):\n",
    "        return response\n",
    "    else:\n",
    "        items = response.json()['items']\n",
    "        reduced_items = map(lambda x: {x['classification']: x['id']}, items)\n",
    "        result = {}\n",
    "        for x in list(reduced_items):\n",
    "            result.update(x)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_level(level: str, json: dict, full: bool = True):\n",
    "\n",
    "    url = base_url + f\"/api/collections/{level}/records\"\n",
    "    headers = {\"Authorization\": POCKETBASE_AUTH_TOKEN}\n",
    "    response = requests.post(url=url, json=json, params={}, headers=headers)\n",
    "    if (full):\n",
    "        return response\n",
    "    else:\n",
    "        return {response.json()['classification']: response.json()['id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions(params: dict = {}, full: bool = True):\n",
    "\n",
    "    url = base_url + \"/api/collections/actions/records\"\n",
    "    headers = {\"Authorization\": POCKETBASE_AUTH_TOKEN}\n",
    "    response = requests.get(url=url, params=params, headers=headers)\n",
    "    if (full):\n",
    "        return response\n",
    "    else:\n",
    "        items = response.json()['items']\n",
    "        reduced_items = map(lambda x: {x['action']: x['id']}, items)\n",
    "        result = {}\n",
    "        for x in list(reduced_items):\n",
    "            result.update(x)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_action(json: dict):\n",
    "\n",
    "    url = base_url + \"/api/collections/actions/records\"\n",
    "    headers = {\"Authorization\": POCKETBASE_AUTH_TOKEN}\n",
    "    response = requests.post(url=url, json=json, params={}, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_action(id: str, json: dict):\n",
    "\n",
    "    url = base_url + f\"/api/collections/actions/records/{id}\"\n",
    "    headers = {\"Authorization\": POCKETBASE_AUTH_TOKEN}\n",
    "    response = requests.patch(url=url, json=json, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_by_name(name: str):\n",
    "\n",
    "    # If the action already exists in the database\n",
    "    if ((pre_action := get_actions(params={\"filter\": f'action=\"{name}\"'}, full=False)) != {}):\n",
    "        return pre_action\n",
    "    # If the action does not exist in the database\n",
    "    else:\n",
    "        data = post_action(json={\"action\": name}).json()\n",
    "        return {data['action']: data['id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eras(params: dict, full: bool = True):\n",
    "\n",
    "    url = base_url + \"/api/collections/eras/records\"\n",
    "    headers = {\"Authorization\": POCKETBASE_AUTH_TOKEN}\n",
    "    response = requests.get(url=url, params=params, headers=headers)\n",
    "    if (full):\n",
    "        return response\n",
    "    else:\n",
    "        items = response.json()['items']\n",
    "        reduced_items = map(lambda x: {x['era']: x['id']}, items)\n",
    "        result = {}\n",
    "        for x in list(reduced_items):\n",
    "            result.update(x)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_era(json: dict):\n",
    "\n",
    "    url = base_url + \"/api/collections/eras/records\"\n",
    "    headers = {\"Authorization\": POCKETBASE_AUTH_TOKEN}\n",
    "    response = requests.post(url=url, json=json, params={}, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_day(json: dict):\n",
    "\n",
    "    url = base_url + '/api/collections/days/records'\n",
    "    headers = {\"Authorization\": POCKETBASE_AUTH_TOKEN}\n",
    "    response = requests.post(url=url, json=json, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_json(filePath):\n",
    "    file = open(filePath)\n",
    "    return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall2021 = read_json(\"../database/weeks_data/json_data/fall2021.json\")\n",
    "spring2022 = read_json(\"../database/weeks_data/json_data/spring2022.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate_database(level, data) will update the database with the actions found\n",
    "# in data. `level` is used to gather the classifications. `data` should be in the \n",
    "# form of a dictionary with keys as classifications for the given `level` and values\n",
    "# as lists of actions.\n",
    "def populate_database(level: str, data: dict):\n",
    "\n",
    "    # Get the current classifications for the level\n",
    "    cats = get_levels(level=level, full=False)\n",
    "\n",
    "    # Loop through all the classifications\n",
    "    for cat in list(data.keys()):\n",
    "\n",
    "        # Get the set of all actions\n",
    "        # actions = set(data[cat][\"weekday\"] + data[cat][\"weekend\"])\n",
    "        actions = set(data[cat])\n",
    "\n",
    "        # Loop through all the actions\n",
    "        for action in actions:\n",
    "\n",
    "            # If the classication in NOT in the level's collection\n",
    "            if (cats.get(cat, 0) == 0):\n",
    "                # Add the classification to the level's collection\n",
    "                new_cat = post_level(level=level, json={\"classification\": cat}, full=False)\n",
    "                # Update the current classifications\n",
    "                cats.update(new_cat)\n",
    "\n",
    "            # If the action is already in the collection\n",
    "            if ((pre_action := get_actions(params={\"filter\": f'action=\"{action}\"'}, full=False)) != {}):\n",
    "                # Patch the action with the current level's classification\n",
    "                patch_action(pre_action[action], json={level: cats[cat]})\n",
    "                continue\n",
    "\n",
    "            # Update the action's collection\n",
    "            post_action({\"action\": action, level: cats[cat]})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the level\n",
    "level = 'level_three'\n",
    "# Get the level's data\n",
    "data = spring2022['classification'][level]['Work']['CS 4411']\n",
    "\n",
    "# populate_database(level=level, data=data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.recap as RP\n",
    "import src.extra as EX\n",
    "import datetime\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eastern_datetime(dt_date: datetime.datetime, time: str):\n",
    "\n",
    "    # Eastern Standard Time\n",
    "    timezone = datetime.timezone(offset=datetime.timedelta(hours=-5), name='EST')\n",
    "\n",
    "    # Check if EST is correct for date range, if false, switch to Eastern Daylight Time (EDT)\n",
    "    check = datetime.datetime.combine(dt_date, datetime.time(tzinfo=timezone)).astimezone()\n",
    "    if (check.tzinfo != timezone):\n",
    "        timezone = datetime.timezone(offset=datetime.timedelta(hours=-4), name='EDT')\n",
    "\n",
    "    # Create datetime time\n",
    "    hours = int(time.split(':')[0])\n",
    "    minutes = int(time.split(':')[1])\n",
    "    dt_time = datetime.time(hour=hours, minute=minutes, tzinfo=timezone)\n",
    "\n",
    "    # Create final datetime\n",
    "    final_dt = datetime.datetime.combine(dt_date, dt_time)\n",
    "\n",
    "    return final_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_date(date: str):\n",
    "\n",
    "    # Create datetime date\n",
    "    return datetime.datetime.strptime(date, \"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_UTC(dt, text) returns a datetime object if `text` is False, or a string format in `text` is True\n",
    "def to_UTC(dt: datetime.datetime, text: bool = False):\n",
    "\n",
    "    if (text):\n",
    "        return dt.astimezone(tz=datetime.timezone(offset=datetime.timedelta(hours=0))).isoformat(' ')[:-6]\n",
    "    else:\n",
    "        return dt.astimezone(tz=datetime.timezone(offset=datetime.timedelta(hours=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes(folder: str, class_links: dict):\n",
    "    \n",
    "    # Defining an empty list to hold strings that represent file locations\n",
    "    weeks_files = []\n",
    "\n",
    "    # For Loop to create strings that represent the file location of exported .csv files from Notion\n",
    "    for x in range(1, 16):\n",
    "        try:\n",
    "            file = f\"../database/weeks_data/{folder}/Week{str(x)}.csv\"\n",
    "            weeks_files.append(file)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Defining an empty list to hold Pandas Dataframe of the previously mentioned Notion .csv files\n",
    "    weeks_df = []\n",
    "\n",
    "    # For Loop to create, and clean multipe Dataframes to represent a week\n",
    "    for x in range(15):\n",
    "        try:\n",
    "            df = pd.read_csv(weeks_files[x])\n",
    "            time = pd.DataFrame(EX.military_time())\n",
    "            df = df.drop(df.columns[[0]], axis=1)\n",
    "            df = pd.concat([time, df], axis=1)\n",
    "            df.columns = pd.Index(['Time', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday','Sunday'], dtype='object')\n",
    "            df = df.replace(class_links)\n",
    "            df = df.replace(to_replace=r'^https://www.notion.so/.*$', value=\"PIKE Meeting\", regex=True)\n",
    "            weeks_df.append(df.fillna(\"Unknown\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return weeks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_links = spring2022['class_link']\n",
    "begin = spring2022['metadata']['date_start']\n",
    "end = spring2022['metadata']['date_end']\n",
    "folder = \"spring2022\"\n",
    "era = \"Spring 2022\"\n",
    "\n",
    "dataframes = create_dataframes(folder=folder, class_links=class_links)\n",
    "\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday','Sunday']\n",
    "\n",
    "split_index = 72\n",
    "\n",
    "eras = get_eras(params={}, full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing date 2022-05-09 03:45:00\n"
     ]
    }
   ],
   "source": [
    "# Define the starting datetime date\n",
    "dt_date = text_to_date(date=begin)\n",
    "\n",
    "# Define pointer variables\n",
    "front_actions = pd.DataFrame() # actions from 00:00 to 05:45\n",
    "back_actions = [] # actions from 06:00 to 23:45\n",
    "\n",
    "# Loop through all weeks\n",
    "for week in dataframes:\n",
    "\n",
    "    past_day = 'Sunday'\n",
    "\n",
    "    # Loop through all days\n",
    "    for day_index, day in enumerate(days):\n",
    "\n",
    "        # Check if there are actions from 00:00 to 05:45\n",
    "        if (front_actions.shape[0] != 0):\n",
    "            \n",
    "            # Loop through all actions from 00:00 to 05:45\n",
    "            for index, time_action in front_actions.iterrows():\n",
    "\n",
    "                # Create time-string\n",
    "                eastern = create_eastern_datetime(dt_date=dt_date, time=time_action['Time'])\n",
    "                utc = to_UTC(dt=eastern, text=True)\n",
    "                # Get action's ID\n",
    "                action_name = time_action[past_day]\n",
    "                action_dict = get_action_by_name(name=action_name)\n",
    "                # Post day with date-string, era_ID, and action_ID\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Processing date {utc}\")\n",
    "                post_day(json={\"date\": utc, \"era\": eras[era], \"action\": action_dict[action_name]})\n",
    "\n",
    "\n",
    "        back_actions = week[['Time', day]].iloc[:split_index]\n",
    "\n",
    "        # Loop through all actions from 06:00 to 23:45\n",
    "        for index, time_action in back_actions.iterrows():\n",
    "\n",
    "            # Create time-string\n",
    "            eastern = create_eastern_datetime(dt_date=dt_date, time=time_action['Time'])\n",
    "            utc = to_UTC(dt=eastern, text=True)\n",
    "            # Get action's ID\n",
    "            action_name = (time_action[day]).replace('\"', \"'\")\n",
    "            action_dict = get_action_by_name(name=action_name)\n",
    "            # Post day with date-string, era_ID, and action_ID\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Processing date {utc}\")\n",
    "            post_day(json={\"date\": utc, \"era\": eras[era], \"action\": action_dict[action_name]})\n",
    "\n",
    "        # Update front_actions\n",
    "        front_actions = week[['Time', day]].iloc[split_index:]\n",
    "        # Update datetime date\n",
    "        dt_date = dt_date + datetime.timedelta(days=1)\n",
    "        # Update past day\n",
    "        past_day = day\n",
    "\n",
    "# Final Loop through front_actions pointer\n",
    "# Loop through all actions from 00:00 to 05:45\n",
    "for index, time_action in front_actions.iterrows():\n",
    "\n",
    "    # Create time-string\n",
    "    eastern = create_eastern_datetime(dt_date=dt_date, time=time_action['Time'])\n",
    "    utc = to_UTC(dt=eastern, text=True)\n",
    "    # Get action's ID\n",
    "    action_name = time_action[past_day]\n",
    "    action_dict = get_action_by_name(name=action_name)\n",
    "    # Post day with date-string, era_ID, and action_ID\n",
    "    post_day(json={\"date\": utc, \"era\": eras[era], \"action\": action_dict[action_name]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe57bcbf9df45ab311c538a2573c53d1e6c2171079d3a8b9afd38917add09bb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
